


**Thoughts on Nov 20** Perhaps we should just consider a threshold concentration or variation. We might not want to keep those volatiles that have such low values. Perhaps just focusing on m31.99, m33.03, m43.01, etc.  

After looking at the data, the first thought is to log transform it. To check for any measurement outliers, we might want to get rid of the zeroes and transform the data to identify potential outliers.

```{r}
long_voc_data %>% 
  filter(conc > 0) %>%
  ggplot(aes(x = voc, y = conc)) + 
  geom_boxplot() +
  theme_bw() + 
  theme(axis.text.x = element_text(angle = 90)) +
  scale_y_log10()
```

With the information, we can clean the data again and remove these outliers. Since the values are so small and below the machine's detection threshold, they are classified as zero.
```{r}
# Making these outliers be zero
voc_data[voc_data < 10e-15] <- 0

long_voc_data <- voc_data %>% 
  gather(., key = "voc", value = "conc", -c(famid, sampid, sex)) %>% 
  mutate(voc = factor(voc, levels = unique(voc)))


```

```{r eval = FALSE}
long_voc_data %>% 
  ggplot(aes(x = voc, y = conc)) + 
  geom_boxplot() +
  theme_bw() + 
  lims(y = c(1e-13, 1e-05)) +
  theme(axis.text.x = element_text(angle = 90)) +
  scale_y_log10()
```

<!--
#### Transformation and scaling

Based on PTR machines detection threshold, any values smaller than `1e-12` are equivalent to zero. Then we will standardize the data.
```{r}
log_vocs <- voc_data %>% 
  mutate_at(vars(starts_with("m")), list(~ log10(. + 1e-12)))

st_log_vocs <- log_vocs %>% 
  mutate_at(vars(starts_with("m")), scale)

#saveRDS(log_vocs, "cleandata/JMP_vocs.RDS")


long_vocs <- st_log_vocs %>% 
  pivot_longer(., -c(famid, sampid, sex),
               names_to = "voc", values_to="conc") %>% 
  #gather(., key = "voc", value = "conc", -c(famid, sampid, sex)) %>% 
  mutate(voc = factor(voc, levels = unique(voc)))

```

This is how the transformed data looks like now, with a visual differentiation between males and females. Recall that the data for VOCs here has been log transfomed and scaled. 
```{r fig.width=24, fig.height=8}
ggarrange(long_vocs %>% 
  ggplot(aes(x = voc, y = conc)) +
  geom_boxplot(notch = TRUE, alpha = 0.2) +
  labs(x = "VOC ID",
       y = "Log transformed and scaled concentration") +
  theme(axis.text.x = element_text(angle = 90)),
  long_vocs %>% 
  ggplot(aes(x = voc, y = conc, fill = sex)) +
  geom_boxplot(notch = TRUE, alpha = 0.2) +
  labs(x = "VOC ID",
       y = "Log transformed and scaled concentration") +
  theme(axis.text.x = element_text(angle = 90)), nrow = 2)
  
```

#### Correlations
The point of running these correlations is to get an idea of which compounds might show up together or the opposite. We are trying to reduce our dataset and avoid inputing 75 dependent variables into a model with two explanatory variables. From the correlation matrix we can see that some compounds tend to be found in clusters. We can use this information to group the compounds and get a reduced dataset.

```{r fig.width=10, fig.height=10}

cor_data <- as.matrix(st_log_vocs[,4:78])

cor_res <- rcorr(x = cor_data, type = "pearson")

saveRDS(cor_res, "cleandata/data_for_figs/voc_correlation_data.RDS")
corrplot(cor_res$r, p.mat = cor_res$P, sig.level = 0.001, insig = "pch", tl.col = "black", tl.srt = 60, tl.cex = .4, order = "hclust", pch.cex = 0.8, pch.col = "#43484f", addrect = 17)
```


#### Clustering

Haven't found a good reasoning to come up with a specific number of clusters. However, by making these clusters using the correlation matrix, we can see how variables (in this case compounds) are grouped together.

```{r}
d <- as.dist(1-cor_res$r)
hr <- hclust(d)
#names(hr)
plot(hr, hang = -1)
rect.hclust(hr, h = max(hr$height)/2)
```

```{r eval = FALSE, echo = FALSE}
# This wa swith arbitrary number of clusters and also not sure about using dist vs as.dist(1-data). Since we are using correlation, I'm incluined to the second option.
voc_dist <- hclust(dist(cor_res$r))
plot(voc_dist, hang = -1)
rect.hclust(voc_dist, k =15)

voc_clusters <- data.frame(clust = cutree(voc_dist, k = 15)) %>% 
  rownames_to_column(var = "voc")
```

Using the clustering to create a new data frame
**Making clusters using the original raw data and then transforming and scaling, after the clusters are formed**  

```{r}
# This uses the original, raw untransformed data to get the means by cluster
voc_clusters <- data.frame(clust = cutree(hr, h = max(hr$height)/2)) %>% 
  rownames_to_column(var = "voc") 


long_voc_data %>% 
  left_join(voc_clusters) %>% 
  group_by(sampid, clust, famid, sex) %>% 
  summarise(voc_value = mean(conc)) %>% 
  mutate(voc_clust = str_pad(clust, width = 2, side = "left", pad = 0),
         voc_clust = paste0("clust_", voc_clust)) %>% 
  ungroup() %>% 
  select(-clust) -> clustered_long_vocs

clustered_long_vocs %>% 
  spread(key = voc_clust, value = voc_value)

clustered_long_vocs %>% 
  pivot_wider(., names_from = "voc_clust", 
               values_from = "voc_value") -> clustered_voc_data


clustered_log_vocs <- clustered_voc_data %>% 
  mutate_at(vars(starts_with("c")), list(~ log10(. + 1e-12)))

clustered_st_log_vocs <- clustered_log_vocs %>% 
  mutate_at(vars(starts_with("c")), scale)

clustered_long_vocs <- clustered_st_log_vocs %>% 
  #gather(., key = "voc", value = "conc", -c(famid, sampid, sex)) %>% 
  pivot_longer(., -c(famid, sampid, sex),
               names_to = "voc", values_to = "conc") %>% 
  mutate(voc = factor(voc, levels = unique(voc)))

clustered_long_vocs %>% 
  ggplot(aes(x = voc, y = conc, fill = sex)) +
  geom_boxplot(notch = TRUE, alpha = 0.2) +
  labs(x = "VOC ID",
       y = "Log transformed and scaled concentration") +
  theme(axis.text.x = element_text(angle = 90))

```
**Making the clusters with the data that was already transformed and scaled**
I think it makes more sense to use this one, since this is the data we actually used to create the clusters. Our correlations and everything else is based on this transformed data, so this is probably what we should use to create the new dataset of clustered VOCs.

```{r}

# This one gets the clustered data by getting the mean using the transformed and scaled data.
voc_clusters <- data.frame(clust = cutree(hr, h = max(hr$height)/2)) %>% 
  rownames_to_column(var = "voc") 


long_vocs %>% 
  left_join(voc_clusters) %>% 
  group_by(sampid, clust, famid, sex) %>% 
  summarise(voc_value = mean(conc)) %>% 
  mutate(voc_clust = str_pad(clust, width = 2, side = "left", pad = 0),
         voc_clust = paste0("clust_", voc_clust)) %>% 
  ungroup() %>% 
  select(-clust) -> clustered_long_vocs2

# clustered_long_vocs2 %>% 
#   spread(key = voc_clust, value = voc_value) -> clustered_voc_data2

clustered_long_vocs2 %>% 
  pivot_wider(., names_from = "voc_clust",
              values_from = "voc_value") ->clustered_voc_data2

clustered_long_vocs2 %>% 
  ggplot(aes(x = voc_clust, y = voc_value, fill = sex)) +
  geom_boxplot(notch = TRUE, alpha = 0.2) +
  labs(x = "VOC ID",
       y = "Log transformed and scaled concentration") +
  theme(axis.text.x = element_text(angle = 90))
```

These are the clusters and the compounds in each cluster
```{r echo = FALSE}
voc_clusters %>% 
  dplyr::select(clust, voc) %>% 
  rename(Cluster = clust, Compound = voc) %>% 
  arrange(Cluster) %>% 
  knitr::kable()
```



### Master dataframe
This will create the master data frame that includes the lifehistory traits and the VOC clustered data. All of these are already log transformed and scaled. Also, there is one row per individual, which means the data for clones has either been averaged(for traits) or the max value has been taken (volatiles).

```{r}
clean_master <- full_join(scaled_traits, clustered_voc_data2) %>% 
  mutate_at(c(5:32), as.numeric)

saveRDS(clean_master, "cleandata/clean_master.RDS")
```

-->